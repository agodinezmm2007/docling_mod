# code_formula_predictor.py
# modified to use smoldocling, retains same class and function names
import logging
from typing import List, Optional, Union

import numpy as np
import torch
from PIL import Image

from latexcleaner import LatexToMarkdown

from transformers import (
    AutoProcessor,
    GenerationConfig,
    AutoModelForVision2Seq,
    StoppingCriteriaList,
    MaxLengthCriteria,
    MaxTimeCriteria,
)

_log = logging.getLogger(__name__)


class CodeFormulaPredictor:
    """
    Equivalent to CodeFormulaPredictor, but uses ds4sd/SmolDocling-256M-preview
    which is an 'idefics3' model (image-text-to-text).

    - label=="code" -> returns plain text code
    - label=="formula" -> returns LaTeX
    """

    def __init__(
        self,
        artifacts_path: str,
        device: str = "cpu",
        num_threads: int = 4,
    ):
        self._device = device
        self._num_threads = num_threads
        if self._device == "cpu":
            torch.set_num_threads(self._num_threads)

        _log.info(f"Loading SmolDocling from {artifacts_path} on {device}...")
        # 1) Use AutoProcessor for Idefics3-based image-text-to-text
        self._processor = AutoProcessor.from_pretrained(artifacts_path)
        self._model = AutoModelForVision2Seq.from_pretrained(
            artifacts_path
        ).to(device)
        self._model.eval()
        _log.info("SmolDoclingPredictor init complete. Device=%s", self._device)
        

    def info(self) -> dict:
        """
        Retrieves configuration details of the CodeFormulaPredictor instance.

        Returns
        -------
        dict
            A dictionary containing configuration details such as the device and
            the number of threads used.
        """
        info = {
            "device": self._device,
            "num_threads": self._num_threads,
        }
        return info

    def _get_prompt(self, label: str) -> str:
        """
        Construct a minimal prompt for code vs. formula bounding-box snippet.
        You can customize if needed, e.g.:

          label=="code":   "<code> snippet "
          label=="formula":"<equation> latex "
        """
        _log.debug("Generating chat message for label: %s", label)
        if label == "code":
            user_text = "Convert this snippet to code text."
        elif label == "formula":
            user_text = "Convert this snippet to LaTeX."
        else:
            user_text = "Describe this snippet."

        return user_text
    
    def _strip(self, text: str) -> str:
        """
        Remove trailing weird tokens, if any.
        """
        remove_list = [" c c c c", " l l l l l", r"\quad", r"\,", r"\\"]

        changed = True
        while changed:
            changed = False
            for substr in remove_list:
                if text.endswith(substr):
                    text = text[: -len(substr)]
                    changed = True
        return text.strip()

    @torch.inference_mode()
    def predict(
        self,
        images: List[Union[Image.Image, np.ndarray]],
        labels: List[str],
        temperature: Optional[float] = 0.0,
        max_generation_time: float = 30.0,
        max_new_tokens: int = 512,
    ) -> List[str]:
        """
        For each bounding-box snippet image, we build an image+prompt input
        and call the Idefics3 model to produce text/LaTeX.

        If label=="formula", post-process the result with LatexToMarkdown.

        Returns a list of decoded outputs.
        """

        if len(images) != len(labels):
            raise ValueError("images and labels must have same length")
        
        _log.info("Starting prediction for %d images with labels: %s", len(images), labels)

        # 1) Set temperature
        try:
            if temperature is None or not isinstance(temperature, (float, int)) or temperature < 0:
                raise Exception("Temperature must be a number greater or equal to 0.")

            do_sample = temperature != 0.0
            actual_temperature = temperature if do_sample else 1.0  # standard practice
        except Exception as te:
            _log.error("Invalid temperature: %s", te, exc_info=True)
            raise

        #---------------------------------------------
        # 2) Convert images => PIL 
        # The docling pipeline gave us PIL images. We'll do a standard resize + normalization:
        #---------------------------------------------
        processed_pil_images = []
        for i, (img, lbl) in enumerate(zip(images, labels)):
            _log.debug("Processing snippet %d: label=%s", i, lbl)
            try:
                if isinstance(img, np.ndarray):
                    img = Image.fromarray(img).convert("RGB")
                elif isinstance(img, Image.Image):
                    img = img.convert("RGB")
                else:
                    raise TypeError(f"Unsupported image type for item {i}")
                processed_pil_images.append(img)
                _log.debug("Processed image %d successfully (label=%s)", i, lbl)
            except Exception as e:
                _log.error("Error processing image %d: %s", i, e, exc_info=True)
                raise

        #---------------------------------------------
        # 3) Build docling chat messages
        #    Each snippet => 1 role='user', with an image + text
        #    We'll store them in parallel arrays: chat_messages, snippet_images
        #---------------------------------------------

        chat_messages = []
        snippet_images = []
        for i, (img, lbl) in enumerate(zip(processed_pil_images, labels)):
            # We'll pass exactly 1 image for each snippet => list-of-1
            # Then the text is e.g. "Convert snippet to code text."
            user_text = self._get_prompt(lbl)

            # Build the docling-like messages structure
            # Example:  {"role":"user","content":[{"type":"image"},{"type":"text","text":"Convert formula..."}]}
            single_message = {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text}
                ]
            }
            chat_messages.append(single_message)
            snippet_images.append([img])

        #---------------------------------------------
        # We'll store final results (like the new snippet's final_texts)
        # but we keep the name "outputs_text" for clarity. 
        #---------------------------------------------
        
        outputs_text: List[str] = []  # <-- CHANGED: new place to store final strings

        #---------------------------------------------
        # 4) For each snippet, do .apply_chat_template => .generate => decode
        #    processor.apply_chat_template([...], add_generation_prompt=True)
        #    Then pass images => [imglist]
        #---------------------------------------------
        
        for i, (msg, imgs) in enumerate(zip(chat_messages, snippet_images)):
            # build conversation with a single message
            # docling can handle multiple messages, but we have only 1 user message
            conversation = [msg]
            prompt_text = self._processor.apply_chat_template(
                conversation, add_generation_prompt=True
            )
            _log.debug("Snippet %d prompt text: %s", i, prompt_text)

        # Create inputs with the current snippet's image and prompt
        inputs = self._processor(
            images=imgs,
            text=prompt_text,
            padding=True,
            truncation=True,
            return_tensors="pt",
        ).to(self._device)

        #---------------------------------------------
        # 5) Define stopping criteria
        # Typically we have input_ids / attention_mask in 'inputs'
        #---------------------------------------------

        if "input_ids" in inputs:
            prompt_ids = inputs["input_ids"]
            _log.debug("Tokenized prompt_ids shape: %s", prompt_ids.shape)
        else:
            prompt_ids = None
            _log.debug("Processor did not produce 'input_ids' (unexpected).")

        # We check if we have prompt_ids for the length, else fallback
        base_len = prompt_ids.shape[1] if prompt_ids is not None else 0
        stopping_criteria = StoppingCriteriaList([
            MaxLengthCriteria(max_length=base_len + max_new_tokens),
            MaxTimeCriteria(max_generation_time),
        ])
        _log.debug(
            "Configured robust stopping criteria (MaxLength: %d tokens, MaxTime: %.2f s).",
            base_len + max_new_tokens, max_generation_time
        )

        #---------------------------------------------
        # 6) Build an advanced GenerationConfig with beam-search 
        # 
        #---------------------------------------------

        generation_config = GenerationConfig(
            do_sample=False,
            num_beams=6,
            num_beam_groups=2,
            early_stopping=True,
            repetition_penalty=1.3,
            length_penalty=0.9,
            diversity_penalty=0.7,
            max_new_tokens=max_new_tokens,
            no_repeat_ngram_size=3,
            use_cache=True,
            temperature=actual_temperature,
        )
        _log.debug("Using GenerationConfig: %s", generation_config.to_dict())

        #---------------------------------------------
        # 7) Generate
        #---------------------------------------------

        if self._device == "cpu":
            output = self._model.generate(
                **inputs,
                generation_config=generation_config,
                stopping_criteria=stopping_criteria,
                output_scores=True,
                return_dict_in_generate=True
            )
        else:
            with torch.autocast(device_type=self._device, dtype=torch.bfloat16):
                output = self._model.generate(
                    **inputs,
                    generation_config=generation_config,
                    stopping_criteria=stopping_criteria,
                    output_scores=True,
                    return_dict_in_generate=True
                )

        output_ids_list = output.sequences
        _log.debug("Generation complete, output_ids_list shape: %s", output_ids_list.shape)

        #---------------------------------------------
        # 8) Log beam scores
        #---------------------------------------------

        if hasattr(output, "sequences_scores") and output.sequences_scores is not None:
            scores = output.sequences_scores.cpu().numpy()
            for beam_i, score in enumerate(scores):
                _log.info("Output %d beam score: %.4f", i, beam_i, score)
        else:
            _log.warning("Beam scores not available. Ensure num_beams>1 and output_scores=True.")

        #---------------------------------------------
        # 9) Decode. multiple beams
        #---------------------------------------------

        for beam_idx in range(output_ids_list.size(0)):
            # If we want to slice off the prompt, do:
            # out_ids = output_ids_list[i, base_len:]
            # text = self._processor.tokenizer.decode(out_ids, skip_special_tokens=True)
            # but let's do a full decode for now:
            out_ids = output_ids_list[beam_idx, base_len:]
            text = self._processor.tokenizer.decode(out_ids, skip_special_tokens=True)
            text = self._strip(text)
     
            #---------------------------------------------
            # 10) Token count check
            #---------------------------------------------

            token_count = len(self._processor.tokenizer.encode(text))
            if token_count >= max_new_tokens:
                _log.warning("Snippet %d, beam %d reached max token limit %d", i, beam_idx, token_count)
            else:
                _log.debug("Snippet %d, beam %d generated %d tokens.", i, beam_idx, token_count)

            #---------------------------------------------
            # 11) clean the formulas
            #---------------------------------------------

            snippet_label = labels[i]  # or you can pass it differently
            if snippet_label == "formula":
                eq_data = [{"latex": text}]
                converter = LatexToMarkdown(eq_data)
                converted = converter.convert_all()
                if converted:
                    text = converted[0]["markdown"]  # e.g. "$$ f_{x} $$"
                    _log.debug("Snippet %d cleaned formula: %s", i, text)
            # Now store this text into outputs_text
            outputs_text.append(text)

        _log.info("Prediction completed successfully. Returning %d total outputs.", len(outputs_text))
        return outputs_text


