# code_formula_predictor.py
# this version uses the original "CodeFormula" model
# Copyright IBM Corp. 2024 - 2024
# SPDX-License-Identifier: MIT
import logging
from typing import List, Optional, Union

import numpy as np
import torch
from PIL import Image
from transformers import( AutoTokenizer, 
                         GenerationConfig, 
                         StoppingCriteriaList,
                         MaxLengthCriteria,
                         MaxTimeCriteria
                         )

from docling_ibm_models.code_formula_model.models.sam_opt import SamOPTForCausalLM
from docling_ibm_models.code_formula_model.models.sam_opt_image_processor import (
    SamOptImageProcessor,
)

_log = logging.getLogger(__name__)

# commented out to test if MaxLengthCriteria and MaxTimeCriteria work better: 03/26/2025 22:22PM
# class StopOnString(StoppingCriteria):
#     def __init__(self, tokenizer, stop_string):
#         self.stop_token_ids = tokenizer.encode(stop_string, add_special_tokens=False)

#     def __call__(self, input_ids, scores, **kwargs):
#         for sequence in input_ids:
#             sequence_list = sequence.tolist()
#             for i in range(len(sequence_list) - len(self.stop_token_ids) + 1):
#                 if (
#                     sequence_list[i : i + len(self.stop_token_ids)]
#                     == self.stop_token_ids
#                 ):
#                     return True
#         return False


class CodeFormulaPredictor:
    """
    Code and Formula Predictor using a multi-modal vision-language model.

    This class enables the prediction of code or LaTeX representations
    from input images of code snippets or mathematical formulas.

    Attributes
    ----------
    _device : str
        The device on which the model is loaded (e.g., 'cpu' or 'cuda').
    _num_threads : int
        Number of threads used for inference when running on CPU.
    _tokenizer : transformers.PreTrainedTokenizer
        Tokenizer for processing textual inputs to the model.
    _model : transformers.PreTrainedModel
        Pretrained multi-modal vision-language model.
    _image_processor : transformers.ImageProcessor
        Processor for normalizing and preparing input images.
    _temperature : float
        Sampling temperature for generation; controls randomness in predictions.
    """

    def __init__(
        self,
        artifacts_path: str,
        device: str = "cpu",
        num_threads: int = 4,
    ):
        """
        Initializes the CodeFormulaPredictor with the specified model artifacts.

        Parameters
        ----------
        artifacts_path : str
            Path to the directory containing the pretrained model files.
        device : str, optional
            Device to run the inference on ('cpu' or 'cuda'), by default "cpu".
        num_threads : int, optional
            Number of threads for CPU inference, by default 4.
        """
        self._device = device
        self._num_threads = num_threads
        if device == "cpu":
            torch.set_num_threads(self._num_threads)

        self._tokenizer = AutoTokenizer.from_pretrained(
            artifacts_path, use_fast=True, padding_side="left"
        )
        self._model = SamOPTForCausalLM.from_pretrained(artifacts_path).to(self._device)
        self._model.eval()

        self._image_processor = SamOptImageProcessor.from_pretrained(artifacts_path)

        _log.debug("CodeFormulaModel settings: {}".format(self.info()))

    def info(self) -> dict:
        """
        Retrieves configuration details of the CodeFormulaPredictor instance.

        Returns
        -------
        dict
            A dictionary containing configuration details such as the device and
            the number of threads used.
        """
        info = {
            "device": self._device,
            "num_threads": self._num_threads,
        }
        return info

    def _get_prompt(self, label: str) -> str:
        """
        Constructs an improved prompt for the model based on the input label.

        Parameters
        ----------
        label : str
            The type of input, either 'code' or 'formula'.

        Returns
        -------
        str
            The constructed prompt, explicitly requesting minimal, pure content.

        Raises
        ------
        NotImplementedError
            If the label is not 'code' or 'formula'.
        """
        _log.debug("Generating prompt for label: %s", label)
        if label == "code":
            query = "<code_image_to_text>"
        elif label == "formula":
            query = "<equation>"
        else:
            _log.error("Invalid label provided: %s", label)
            raise NotImplementedError("Label must be either code or formula")

        # Add a short system-like instruction before we drop in the “image” token block:
        prompt = (
            "A chat between a curious user and an artificial intelligence"
            " assistant. The assistant gives helpful, detailed, and polite answers to"
            " the user's questions. USER: "
        )
        prompt += (
            "<img>" + "<imgpad>" * 256 + "</img>" + "\n" + " ASSISTANT:" + "\n" + query
        )
        
        _log.debug("Generated prompt: %s", prompt)
        return prompt

    def _strip(self, text: str):
        """
        Removes any occurrences of the substrings in remove_list from the end of text.

        Parameters
        ----------
        text : str
            The original string.

        Returns
        -------
        str
            The trimmed string.
        """
        remove_list = [r"\quad", r"\\", r"\,", " c c c c", " l l l l l"]
        changed = True
        while changed:
            changed = False
            for substr in remove_list:
                if text.endswith(substr):
                    text = text[: -len(substr)]
                    changed = True

        return text.strip()

    @torch.inference_mode()
    def predict(
        self,
        images: List[Union[Image.Image, np.ndarray]],
        labels: List[str],
        temperature: Optional[float] = 0.0,
        max_generation_time: float = 30.0,  # Robust timeout
        max_new_tokens: int = 512,  # Limit for runaway tokens
    ) -> List[str]:
        """
        Predicts the textual representation of input images (code or LaTeX).

        Parameters
        ----------
        images : List[Union[Image.Image, np.ndarray]]
            List of images to be processed, provided as PIL Image objects or numpy arrays.
        labels : List[str]
            List of labels indicating the type of each image ('code' or 'formula').
        temperature : Optional[float]
            Sampling temperature for generation, by default set to 0.0.

        Returns
        -------
        List[str]
            List of predicted textual outputs for each input image in the given input
            order.

        Raises
        ------
        TypeError
            If any of the input images is not of a supported type (PIL Image or numpy array).
        Exception
            In case the temperature is an invalid number.

        Modifications
        -------------
        [03/26/2025]
            - Implemented robust stopping criteria using `MaxLengthCriteria` and `MaxTimeCriteria`
            for controlled generation, replacing the previous custom string-based stopping criteria.
            - Updated generation parameters (`max_new_tokens` set explicitly to 512 and
            `no_repeat_ngram_size` reduced from 200 to 20) to mitigate runaway token generation issues.
            - Integrated Hugging Face's `GenerationConfig` for a structured and maintainable definition
            of model generation parameters.
            - Enhanced logging to clearly indicate generation termination conditions (timeouts or token limits),
            aiding debugging and monitoring.
        """
        _log.info("Starting prediction for %d images with labels: %s", len(images), labels)
        try:
            if temperature is None or not isinstance(temperature, (float, int)) or temperature < 0:
                raise Exception("Temperature must be a number greater or equal to 0.")

            do_sample = temperature != 0.0
            actual_temperature = temperature if do_sample else 1.0  # standard practice

            if len(labels) != len(images):
                raise Exception("The number of images must be the same as the number of labels.")

            images_tmp = []
            for idx, image in enumerate(images):
                try:
                    if isinstance(image, Image.Image):
                        image = image.convert("RGB")
                    elif isinstance(image, np.ndarray):
                        image = Image.fromarray(image).convert("RGB")
                    else:
                        raise TypeError("Not supported input image format")
                    images_tmp.append(image)
                    _log.debug("Processed image %d successfully", idx)
                except Exception as e:
                    _log.error("Error processing image %d: %s", idx, e, exc_info=True)
                    raise

            images_tensor = torch.stack(
                [self._image_processor(img) for img in images_tmp]
            ).to(self._device)
            
            _log.debug("Created images tensor with shape: %s", images_tensor.shape)

            prompts = [self._get_prompt(label) for label in labels]
            # _log.debug("Prompts: %s", prompts)

            tokenized = self._tokenizer(prompts, padding=True, return_tensors="pt")
            tokenized = {k: v.to(self._device) for k, v in tokenized.items()}

            prompt_ids = tokenized["input_ids"]
            attention_mask = tokenized["attention_mask"]
            _log.debug("Tokenized prompt_ids shape: %s", prompt_ids.shape)

            ##############################################
            # swapping to GenerationConfig 3/27/2025 00:06AM
            ##############################################
            # Robust stopping criteria implementation:
            stopping_criteria = StoppingCriteriaList([
                MaxLengthCriteria(max_length=prompt_ids.shape[1] + max_new_tokens),
                MaxTimeCriteria(max_generation_time)
            ])
            _log.debug("Configured robust stopping criteria (MaxLength: %d tokens, MaxTime: %.2f seconds).", 
                    prompt_ids.shape[1] + max_new_tokens, max_generation_time)

            # Use GenerationConfig for better readability and maintainability
            # Restore complete GenerationConfig with detailed beam search parameters:
            generation_config = GenerationConfig(
                do_sample=False,                 # Beam search is deterministic
                num_beams=6,                     # Optimal balance between speed and diversity
                num_beam_groups=2,               # strictly > 1
                early_stopping=True,             # Stops early once suitable output is found
                repetition_penalty=1.3,          # Penalize repetitive token sequences
                length_penalty=0.9,              # Slightly prefer concise outputs
                diversity_penalty=0.7,           # Improves diversity across beams
                max_new_tokens=max_new_tokens,   # Prevent runaway generation
                no_repeat_ngram_size=3,          # Prevent repetitive sequences explicitly
                use_cache=True,                  # Enable caching for efficiency
                temperature=actual_temperature,  # Explicitly include the temperature setting
            )
            _log.debug("Restored complete GenerationConfig with beam search: %s", generation_config.to_dict())

            if self._device == "cpu":
                output = self._model.generate(
                    input_ids=prompt_ids,
                    attention_mask=attention_mask,
                    images=images_tensor,
                    generation_config=generation_config,  # pass everything else via GenerationConfig
                    stopping_criteria=stopping_criteria,  # pass stopping criteria here instead
                    output_scores=True,                 # <-- Enables access to beam/token scores
                    return_dict_in_generate=True        # <-- Required to access scores
                )
            else:
                with torch.autocast(device_type=self._device, dtype=torch.bfloat16):
                    output = self._model.generate(
                        input_ids=prompt_ids,
                        attention_mask=attention_mask,
                        images=images_tensor,
                        generation_config=generation_config,  # pass everything else via GenerationConfig
                        stopping_criteria=stopping_criteria,  # pass stopping criteria here instead
                        output_scores=True,                 # <-- Enables access to beam/token scores
                        return_dict_in_generate=True        # <-- Required to access scores
                    )
                    
            output_ids_list = output.sequences

            _log.debug("Generation complete, output shape: %s", output_ids_list.shape)
            outputs = self._tokenizer.batch_decode(
                output_ids_list[:, prompt_ids.shape[1]:], skip_special_tokens=True
            )
            outputs = [self._strip(output) for output in outputs]

            # Log beam scores clearly and correctly:
            if hasattr(output, 'sequences_scores') and output.sequences_scores is not None:
                scores = output.sequences_scores.cpu().numpy()
                for idx, score in enumerate(scores):
                    _log.info("Output %d beam score: %.4f", idx, score)
            else:
                _log.warning("Beam scores not available. Ensure num_beams>1 and output_scores=True.")

            # Additional logging about generation length and potential truncation
            for idx, output in enumerate(outputs):
                token_count = len(self._tokenizer.encode(output))
                if token_count >= max_new_tokens:
                    _log.warning("Output %d reached the maximum token limit (%d tokens).", idx, token_count)
                else:
                    _log.debug("Output %d generated %d tokens.", idx, token_count)

            _log.info("Prediction completed successfully.")
            return outputs
        
        except Exception as e:
            _log.error("Error in predict(): %s", e, exc_info=True)
            raise

            ###################################################
            # Rollback code down below, just select all the way from the line below this one (the one with all the ######) till the line with 
            # "swapping to GenerationConfig 3/27/2025 00:06AM" and delete to rollback
            ##################################################

            # commented out to test if MaxLengthCriteria and MaxTimeCriteria work better: 03/26/2025 22:22PM
            # stopping_criteria = StoppingCriteriaList(
            #     [
            #         StopOnString(self._tokenizer, r" \quad \quad \quad \quad"),
            #         StopOnString(self._tokenizer, r" \\ \\ \\ \\"),
            #         StopOnString(self._tokenizer, r" \, \, \, \,"),
            #         StopOnString(self._tokenizer, r" c c c c c c c c c c c c c c c c"),
            #         StopOnString(self._tokenizer, r" l l l l l l l l l l l l l l l l l"),
            #     ]
            # )
            # _log.debug("Configured stopping criteria.")

        #     if self._device == "cpu":
        #         output_ids_list = self._model.generate(
        #             input_ids=prompt_ids,
        #             attention_mask=attention_mask,
        #             images=images_tensor,
        #             do_sample=do_sample,
        #             temperature=temperature,
        #             max_new_tokens=4096 - prompt_ids.shape[1],
        #             use_cache=True,
        #             no_repeat_ngram_size=200,
        #             stopping_criteria=stopping_criteria,
        #         )
        #     else:
        #         with torch.autocast(device_type=self._device, dtype=torch.bfloat16):
        #             output_ids_list = self._model.generate(
        #                 prompt_ids,
        #                 images=images_tensor,
        #                 do_sample=do_sample,
        #                 temperature=temperature,
        #                 max_new_tokens=4096 - prompt_ids.shape[1],
        #                 use_cache=True,
        #                 no_repeat_ngram_size=200,
        #                 stopping_criteria=stopping_criteria,
        #             )

        #     _log.debug("Generation complete, output shape: %s", output_ids_list.shape)
        #     outputs = self._tokenizer.batch_decode(
        #         output_ids_list[:, prompt_ids.shape[1] :], skip_special_tokens=True
        #     )
        #     outputs = [self._strip(output) for output in outputs]
        #     _log.info("Prediction completed successfully.")
        #     return outputs
        
        # except Exception as e:
        #     _log.error("Error in predict(): %s", e, exc_info=True)
        #     raise

# ORIGINAL STOPPING CRITERIA CODE BELOW (COMMENTED OUT FOR BACKUP)
# class StopOnString(StoppingCriteria):
#     def __init__(self, tokenizer, stop_string):
#         self.stop_token_ids = tokenizer.encode(stop_string, add_special_tokens=False)
#     def __call__(self, input_ids, scores, **kwargs):
#         for sequence in input_ids:
#             sequence_list = sequence.tolist()
#             for i in range(len(sequence_list) - len(self.stop_token_ids) + 1):
#                 if sequence_list[i : i + len(self.stop_token_ids)] == self.stop_token_ids:
#                     return True
#         return False
#
# stopping_criteria = StoppingCriteriaList([
#     StopOnString(self._tokenizer, r" \quad \quad \quad \quad"),
#     StopOnString(self._tokenizer, r" \\ \\ \\ \\"),
#     StopOnString(self._tokenizer, r" \, \, \, \,"),
#     StopOnString(self._tokenizer, r" c c c c c c c c c c c c c c c c"),
#     StopOnString(self._tokenizer, r" l l l l l l l l l l l l l l l l l"),
# ])