# modified docling_ibm_models\layoutmodel\layout_predictor.py
# Copyright IBM Corp. 2024 - 2024
# SPDX-License-Identifier: MIT
#
import logging
import os
from collections.abc import Iterable
from typing import Set, Union

import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from transformers import RTDetrForObjectDetection, RTDetrImageProcessor

_log = logging.getLogger(__name__)


class LayoutPredictor:
    """
    Document layout prediction using safe tensors
    """

    def __init__(
        self,
        artifact_path: str,
        device: str = "cpu",
        num_threads: int = 4,
        base_threshold: float = 0.3,
        blacklist_classes: Set[str] = set(),
    ):
        """
        Provide the artifact path that contains the LayoutModel file

        Parameters
        ----------
        artifact_path: Path for the model torch file.
        device: (Optional) device to run the inference.
        num_threads: (Optional) Number of threads to run the inference if device = 'cpu'

        Raises
        ------
        FileNotFoundError when the model's torch file is missing
        """
        # Initialize classes map:
        self._classes_map = {
            0: "background",
            1: "Caption",
            2: "Footnote",
            3: "Formula",
            4: "List-item",
            5: "Page-footer",
            6: "Page-header",
            7: "Picture",
            8: "Section-header",
            9: "Table",
            10: "Text",
            11: "Title",
            12: "Document Index",
            13: "Code",
            14: "Checkbox-Selected",
            15: "Checkbox-Unselected",
            16: "Form",
            17: "Key-Value Region",
        }

        # Blacklisted classes
        self._black_classes = blacklist_classes  # set(["Form", "Key-Value Region"])

        # Set basic params
        self._threshold = base_threshold  # Score threshold
        self._image_size = 640
        self._size = np.asarray([[self._image_size, self._image_size]], dtype=np.int64)

        # Set number of threads for CPU
        self._device = torch.device(device)
        self._num_threads = num_threads
        if device == "cpu":
            torch.set_num_threads(self._num_threads)

        # Model file and configurations
        self._st_fn = os.path.join(artifact_path, "model.safetensors")
        if not os.path.isfile(self._st_fn):
            raise FileNotFoundError("Missing safe tensors file: {}".format(self._st_fn))

        # Load model and move to device
        processor_config = os.path.join(artifact_path, "preprocessor_config.json")
        model_config = os.path.join(artifact_path, "config.json")
        self._image_processor = RTDetrImageProcessor.from_json_file(processor_config)
        self._model = RTDetrForObjectDetection.from_pretrained(
            artifact_path, config=model_config
        ).to(self._device)
        self._model.eval()

        _log.debug("LayoutPredictor settings: {}".format(self.info()))

    def info(self) -> dict:
        """
        Get information about the configuration of LayoutPredictor
        """
        info = {
            "safe_tensors_file": self._st_fn,
            "device": self._device.type,
            "num_threads": self._num_threads,
            "image_size": self._image_size,
            "threshold": self._threshold,
        }
        return info

    @torch.inference_mode()
    def predict(self, orig_img: Union[Image.Image, np.ndarray]) -> Iterable[dict]:
        if isinstance(orig_img, np.ndarray):
            orig_img = Image.fromarray(orig_img)

        orig_img = orig_img.convert("RGB")

        original_width, original_height = orig_img.size

        # Hugging Face preprocessor handles resizing based on preprocessor_config.json
        inputs = self._image_processor(
            images=orig_img,
            return_tensors="pt",
            size={"height": self._image_size, "width": self._image_size},
        ).to(self._device)

        outputs = self._model(**inputs)

        # Accurate bounding-box mapping back to original image dimensions
        target_sizes = torch.tensor([[original_height, original_width]], device=self._device)
        results = self._image_processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=self._threshold
        )

        result = results[0]
        boxes = result["boxes"].cpu().numpy()
        scores = result["scores"].cpu().numpy()
        labels = result["labels"].cpu().numpy()

        for score, label_id, box in zip(scores, labels, boxes):
            label_str = self._classes_map[label_id + 1]
            if label_str in self._black_classes:
                continue

            l, t, r, b = box
            yield {
                "l": float(l),
                "t": float(t),
                "r": float(r),
                "b": float(b),
                "label": label_str,
                "confidence": float(score),
            }
